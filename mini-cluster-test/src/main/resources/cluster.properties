#
# Build Info (Generated by Build Process - Do Not Change)
# ----------------------------------------------------------------------------------------------------
info.build.version=${project.version}

# Hadoop Configuration
# ----------------------------------------------------------------------------------------------------
# Hadoop user that will run job
hadoopUser=devuser

# AEL Spark Properties
# ----------------------------------------------------------------------------------------------------
# Main PDI driver location (can be pdi-ee-client or pdi-spark-driver)
sparkApp=/home/fcamara/@work/executor-8.1-QAT-293/data-integration

# Spark Configuration
# ----------------------------------------------------------------------------------------------------
# Location of Spark client(s) distribution
sparkHome=/home/fcamara/Spark/2.2.0.cloudera2
sparkHome2.1.0=/home/fcamara/Spark/spark-2.1.0-bin-hadoop2.7
sparkHome2.2.0=/home/fcamara/Spark/spark-2.2.0-bin-hadoop2.7
sparkHome2.3.0=/home/fcamara/Spark/spark-2.3.0-bin-hadoop2.7
sparkHome2.1.0.cloudera1=/home/fcamara/Spark/2.1.0.cloudera1
sparkHome2.2.0.cloudera2=/home/fcamara/Spark/2.2.0.cloudera2

# Deploy Mode
# Only used when the master is set to `yarn`.
# client  - Driver will run on the daemon machine; but the executors will run in Yarn
# cluster - Entire application will be run in the cluster (Not supported yet)
sparkDeployMode=client

# Spark Debug Properties
# ----------------------------------------------------------------------------------------------------
# If set > -1 Debugging will be enabled for the Spark application.
driverDebugPort=-1
executorDebugPort=-1

# If true the Spark application will wait for a debugger to attach before executing.
suspendDebug=false

# Add extra Java options for the Spark driver application.
sparkDriverExtraJavaOptions=-Dlog4j.configuration=file:/home/fcamara/@work/executor-8.1-QAT-293/data-integration/classes/log4j.xml

# Add extra Java options for the Spark executors.
sparkExecutorExtraJavaOptions=-Dlog4j.configuration=file:/home/fcamara/@work/executor-8.1-QAT-293/data-integration/classes/log4j.xml

# The amount of memory to allocate to the Spark driver (1g, 2g, 8g, ...).
sparkDriverMemory=4g

# The amount of memory to allocate to the Spark executor (1g, 2g, 8g, ...).
sparkExecutorMemory=1g

# Spark History settings - enables event logging so that transformations can be viewed in the Spark History UI
# ----------------------------------------------------------------------------------------------------
sparkEventLogEnabled=false
#sparkEventLogDir=<event-log-dir>

# Hadoop common test jar path
sparkDistClasspath=/home/fcamara/jars/hadoop-common-test-0.22.0.jar
#example when running vendor spark distro
# sparkDistClasspath=/home/fcamara/jars/guava-17.0.jar:/home/fcamara/jars/hadoop-common-test-0.22.0.jar:/home/fcamara/jars/jackson-mapper-asl-1.9.13.jar:/home/fcamara/jars/jackson-core-asl-1.9.13.jar:/etc/hadoop/conf:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop/lib/*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop/.//*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop-hdfs/./:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop-hdfs/lib/*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop-hdfs/.//*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop-yarn/lib/*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop/libexec/../../hadoop-yarn/.//*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop-mapreduce/lib/*:/home/fcamara/Spark/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/hadoop-mapreduce/.//*


# Mini Cluster settings - AEL, HDFS, Yarn properties
# ----------------------------------------------------------------------------------------------------
# AEL - PDI Spark Executor zip file. This file will upload to HDFS when starting the Mini Cluster
executorZip=/home/fcamara/@work/executor-8.1-QAT-293/pdi-spark-executor.zip

# true to clean up temporary files in /tmp : spark configuration file, yarn_site.xml and results file
cleanup=true

# HDFS
hdfs.namenode.port=20112
hdfs.namenode.http.port=50070
hdfs.temp.dir=/tmp/embedded_hdfs
hdfs.num.datanodes=1
hdfs.enable.permissions=false
hdfs.format=true
hdfs.enable.running.user.as.proxy.user=true
hdfs.cleanup=true

# YARN
yarn.num.node.managers=1
yarn.num.local.dirs=1
yarn.num.log.dirs=1
yarn.resource.manager.address=localhost:37001
yarn.resource.manager.hostname=localhost
yarn.resource.manager.scheduler.address=localhost:37002
yarn.resource.manager.resource.tracker.address=localhost:37003
yarn.resource.manager.webapp.address=localhost:37004
yarn.cleanup=true


# Specific spark / yarn properties
# ----------------------------------------------------------------------------------------------------
#spark.blockManager.port 38000
#spark.broadcast.port 38001
#spark.driver.port 38002
#spark.executor.port 38003
#spark.fileserver.port 38004
#spark.replClassServer.port 38005
#spark.yarn.archive=/home/fcamara/git/pentaho-ee/adaptive-execution/integration-test/tests/src/test/resources/spark-relases/spark-assembly-apache2.1.0.zip